{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "ba1bcb9ff17f4553a81db1e785809888": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c60f6cadc2c0473291bb31561da8aa75",
       "IPY_MODEL_1e34c1ae66b0467c9661c11d4da53fee",
       "IPY_MODEL_784514e8b24c41749273b51c9910ac88"
      ],
      "layout": "IPY_MODEL_76b3ffa319034c47a1aeab12ac3fad2e"
     }
    },
    "c60f6cadc2c0473291bb31561da8aa75": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae1d58b642c94419a1a28856cb11ddc3",
      "placeholder": "​",
      "style": "IPY_MODEL_dbbd63c1b01549198ec9b797bb55c80e",
      "value": "llama-2-13b-chat.ggmlv3.q5_1.bin: 100%"
     }
    },
    "1e34c1ae66b0467c9661c11d4da53fee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c76ef761f92449138197258ce57ef88b",
      "max": 9763701888,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e375052eea04371b6698a608139a915",
      "value": 9763701888
     }
    },
    "784514e8b24c41749273b51c9910ac88": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b92920245f0425c8ea88e17d47d5232",
      "placeholder": "​",
      "style": "IPY_MODEL_f043188c15454e90bd3c93cfd55d33bd",
      "value": " 9.76G/9.76G [01:33&lt;00:00, 41.4MB/s]"
     }
    },
    "76b3ffa319034c47a1aeab12ac3fad2e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae1d58b642c94419a1a28856cb11ddc3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbbd63c1b01549198ec9b797bb55c80e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c76ef761f92449138197258ce57ef88b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e375052eea04371b6698a608139a915": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1b92920245f0425c8ea88e17d47d5232": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f043188c15454e90bd3c93cfd55d33bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#**Llama 2**"
   ],
   "metadata": {
    "id": "4bKQIsIq-d8y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Llama 2 is a collection of pretrained and fine-tuned generative text models, ranging from 7 billion to 70 billion parameters, designed for dialogue use cases."
   ],
   "metadata": {
    "id": "PnV5UC7A2vBZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " It outperforms open-source chat models on most benchmarks and is on par with popular closed-source models in human evaluations for helpfulness and safety."
   ],
   "metadata": {
    "id": "AC41zK5l3Abp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Llama 2 13B-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)"
   ],
   "metadata": {
    "id": "4nobX9E83PjQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`llama.cpp`'s objective is to run the LLaMA model with 4-bit integer quantization on MacBook. It is a plain C/C++ implementation optimized for Apple silicon and x86 architectures, supporting various integer quantization and BLAS libraries. Originally a web chat example, it now serves as a development playground for ggml library features.\n",
    "\n",
    "`GGML`, a C library for machine learning, facilitates the distribution of large language models (LLMs). It utilizes quantization to enable efficient LLM execution on consumer hardware. GGML files contain binary-encoded data, including version number, hyperparameters, vocabulary, and weights. The vocabulary comprises tokens for language generation, while the weights determine the LLM's size. Quantization reduces precision to optimize resource usage."
   ],
   "metadata": {
    "id": "0K4QuEDH4CbY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Quantized Models from the Hugging Face Community"
   ],
   "metadata": {
    "id": "3YC846SH5DOK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n",
    "\n",
    "There are several variations available, but the ones that interest us are based on the GGLM library.\n",
    "\n",
    "We can see the different variations that Llama-2-13B-GGML has [here](https://huggingface.co/models?search=llama%202%20ggml).\n",
    "\n",
    "\n",
    "\n",
    "In this case, we will use the model called [Llama-2-13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)."
   ],
   "metadata": {
    "id": "0TD82wis5LGA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#**Step 1: Install All the Required Packages**"
   ],
   "metadata": {
    "id": "YQZBmz7I5neU"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0avf7xx2lcj",
    "outputId": "8e695f0b-215e-4fb9-ddb1-776c903e10e3",
    "ExecuteTime": {
     "end_time": "2024-04-27T16:43:42.837352Z",
     "start_time": "2024-04-27T16:42:03.642765Z"
    }
   },
   "source": [
    "# GPU llama-cpp-python\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "!pip install huggingface_hub\n",
    "!pip install llama-cpp-python==0.1.78\n",
    "!pip install numpy==1.23.4"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 23.2.1 from /home/lerceg/rn-eestech/.venv/lib/python3.11/site-packages/pip (python 3.11)\r\n",
      "Collecting llama-cpp-python==0.1.78\r\n",
      "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m870.3 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25h  Running command pip subprocess to install build dependencies\r\n",
      "  Collecting setuptools>=42\r\n",
      "    Obtaining dependency information for setuptools>=42 from https://files.pythonhosted.org/packages/f7/29/13965af254e3373bceae8fb9a0e6ea0d0e571171b80d6646932131d6439b/setuptools-69.5.1-py3-none-any.whl.metadata\r\n",
      "    Using cached setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\r\n",
      "  Collecting scikit-build>=0.13\r\n",
      "    Obtaining dependency information for scikit-build>=0.13 from https://files.pythonhosted.org/packages/fa/af/b3ef8fe0bb96bf7308e1f9d196fc069f0c75d9c74cfaad851e418cc704f4/scikit_build-0.17.6-py3-none-any.whl.metadata\r\n",
      "    Using cached scikit_build-0.17.6-py3-none-any.whl.metadata (14 kB)\r\n",
      "  Collecting cmake>=3.18\r\n",
      "    Obtaining dependency information for cmake>=3.18 from https://files.pythonhosted.org/packages/be/2e/8f8daa34e6d743816f852c752da544c34ead6b4fbbdf547519430bdba590/cmake-3.29.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "    Using cached cmake-3.29.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\r\n",
      "  Collecting ninja\r\n",
      "    Obtaining dependency information for ninja from https://files.pythonhosted.org/packages/6d/92/8d7aebd4430ab5ff65df2bfee6d5745f95c004284db2d8ca76dcbfd9de47/ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata\r\n",
      "    Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\r\n",
      "  Collecting distro (from scikit-build>=0.13)\r\n",
      "    Obtaining dependency information for distro from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\r\n",
      "    Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "  Collecting packaging (from scikit-build>=0.13)\r\n",
      "    Obtaining dependency information for packaging from https://files.pythonhosted.org/packages/49/df/1fceb2f8900f8639e278b056416d49134fb8d84c5942ffaa01ad34782422/packaging-24.0-py3-none-any.whl.metadata\r\n",
      "    Using cached packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\r\n",
      "  Collecting wheel>=0.32.0 (from scikit-build>=0.13)\r\n",
      "    Obtaining dependency information for wheel>=0.32.0 from https://files.pythonhosted.org/packages/7d/cd/d7460c9a869b16c3dd4e1e403cce337df165368c71d6af229a74699622ce/wheel-0.43.0-py3-none-any.whl.metadata\r\n",
      "    Using cached wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "  Using cached setuptools-69.5.1-py3-none-any.whl (894 kB)\r\n",
      "  Using cached scikit_build-0.17.6-py3-none-any.whl (84 kB)\r\n",
      "  Using cached cmake-3.29.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\r\n",
      "  Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\r\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\r\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\r\n",
      "  Using cached packaging-24.0-py3-none-any.whl (53 kB)\r\n",
      "  Installing collected packages: ninja, wheel, setuptools, packaging, distro, cmake, scikit-build\r\n",
      "  Successfully installed cmake-3.29.2 distro-1.9.0 ninja-1.11.1.1 packaging-24.0 scikit-build-0.17.6 setuptools-69.5.1 wheel-0.43.0\r\n",
      "\r\n",
      "  \u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "  \u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\r\n",
      "  Running command Getting requirements to build wheel\r\n",
      "  running egg_info\r\n",
      "  writing llama_cpp_python.egg-info/PKG-INFO\r\n",
      "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\r\n",
      "  writing requirements to llama_cpp_python.egg-info/requires.txt\r\n",
      "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\r\n",
      "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\r\n",
      "  adding license file 'LICENSE.md'\r\n",
      "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\r\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\r\n",
      "  Running command Preparing metadata (pyproject.toml)\r\n",
      "  running dist_info\r\n",
      "  creating /tmp/pip-modern-metadata-c5_e__26/llama_cpp_python.egg-info\r\n",
      "  writing /tmp/pip-modern-metadata-c5_e__26/llama_cpp_python.egg-info/PKG-INFO\r\n",
      "  writing dependency_links to /tmp/pip-modern-metadata-c5_e__26/llama_cpp_python.egg-info/dependency_links.txt\r\n",
      "  writing requirements to /tmp/pip-modern-metadata-c5_e__26/llama_cpp_python.egg-info/requires.txt\r\n",
      "  writing top-level names to /tmp/pip-modern-metadata-c5_e__26/llama_cpp_python.egg-info/top_level.txt\r\n",
      "  writing manifest file '/tmp/pip-modern-metadata-c5_e__26/llama_cpp_python.egg-info/SOURCES.txt'\r\n",
      "  reading manifest file '/tmp/pip-modern-metadata-c5_e__26/llama_cpp_python.egg-info/SOURCES.txt'\r\n",
      "  adding license file 'LICENSE.md'\r\n",
      "  writing manifest file '/tmp/pip-modern-metadata-c5_e__26/llama_cpp_python.egg-info/SOURCES.txt'\r\n",
      "  creating '/tmp/pip-modern-metadata-c5_e__26/llama_cpp_python-0.1.78.dist-info'\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\r\n",
      "  Link requires a different Python (3.11.6 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/3a/be/650f9c091ef71cb01d735775d554e068752d3ff63d7943b26316dc401749/numpy-1.21.2.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\r\n",
      "  Link requires a different Python (3.11.6 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/5f/d6/ad58ded26556eaeaa8c971e08b6466f17c4ac4d786cd3d800e26ce59cc01/numpy-1.21.3.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\r\n",
      "  Link requires a different Python (3.11.6 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/fb/48/b0708ebd7718a8933f0d3937513ef8ef2f4f04529f1f66ca86d873043921/numpy-1.21.4.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\r\n",
      "  Link requires a different Python (3.11.6 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/c2/a8/a924a09492bdfee8c2ec3094d0a13f2799800b4fdc9c890738aeeb12c72e/numpy-1.21.5.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\r\n",
      "  Link requires a different Python (3.11.6 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/45/b7/de7b8e67f2232c26af57c205aaad29fe17754f793404f59c8a730c7a191a/numpy-1.21.6.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\r\n",
      "Collecting numpy==1.23.4\r\n",
      "  Obtaining dependency information for numpy==1.23.4 from https://files.pythonhosted.org/packages/5a/7b/653f2c23240e80a560f3043bfe94f7d3804badf969b89273d8ab141133d9/numpy-1.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Downloading numpy-1.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\r\n",
      "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.78)\r\n",
      "  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/01/f3/936e209267d6ef7510322191003885de524fc48d1b43269810cd589ceaf5/typing_extensions-4.11.0-py3-none-any.whl.metadata\r\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\r\n",
      "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\r\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\r\n",
      "Downloading numpy-1.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m17.1/17.1 MB\u001B[0m \u001B[31m780.9 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.5/45.5 kB\u001B[0m \u001B[31m1.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m-:--:--\u001B[0m\r\n",
      "\u001B[?25hDownloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\r\n",
      "Building wheels for collected packages: llama-cpp-python\r\n",
      "  Running command Building wheel for llama-cpp-python (pyproject.toml)\r\n",
      "\r\n",
      "\r\n",
      "  --------------------------------------------------------------------------------\r\n",
      "  -- Trying 'Ninja' generator\r\n",
      "  --------------------------------\r\n",
      "  ---------------------------\r\n",
      "  ----------------------\r\n",
      "  -----------------\r\n",
      "  ------------\r\n",
      "  -------\r\n",
      "  --\r\n",
      "  \u001B[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\r\n",
      "    Compatibility with CMake < 3.5 will be removed from a future version of\r\n",
      "    CMake.\r\n",
      "\r\n",
      "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\r\n",
      "    CMake that the project does not need compatibility with older versions.\r\n",
      "\r\n",
      "  \u001B[0mNot searching for unused variables given on the command line.\r\n",
      "\r\n",
      "  -- The C compiler identification is GNU 13.2.0\r\n",
      "  -- Detecting C compiler ABI info\r\n",
      "  -- Detecting C compiler ABI info - done\r\n",
      "  -- Check for working C compiler: /usr/bin/cc - skipped\r\n",
      "  -- Detecting C compile features\r\n",
      "  -- Detecting C compile features - done\r\n",
      "  -- The CXX compiler identification is GNU 13.2.0\r\n",
      "  -- Detecting CXX compiler ABI info\r\n",
      "  -- Detecting CXX compiler ABI info - done\r\n",
      "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n",
      "  -- Detecting CXX compile features\r\n",
      "  -- Detecting CXX compile features - done\r\n",
      "  -- Configuring done (0.3s)\r\n",
      "  -- Generating done (0.0s)\r\n",
      "  -- Build files have been written to: /tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8/_cmake_test_compile/build\r\n",
      "  --\r\n",
      "  -------\r\n",
      "  ------------\r\n",
      "  -----------------\r\n",
      "  ----------------------\r\n",
      "  ---------------------------\r\n",
      "  --------------------------------\r\n",
      "  -- Trying 'Ninja' generator - success\r\n",
      "  --------------------------------------------------------------------------------\r\n",
      "\r\n",
      "  Configuring Project\r\n",
      "    Working directory:\r\n",
      "      /tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8/_skbuild/linux-x86_64-3.11/cmake-build\r\n",
      "    Command:\r\n",
      "      /tmp/pip-build-env-0w0ekvmf/overlay/lib/python3.11/site-packages/cmake/data/bin/cmake /tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8 -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-0w0ekvmf/overlay/lib/python3.11/site-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8/_skbuild/linux-x86_64-3.11/cmake-install -DPYTHON_VERSION_STRING:STRING=3.11.6 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-0w0ekvmf/overlay/lib/python3.11/site-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/home/lerceg/rn-eestech/.venv/bin/python -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.11 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.11.so -DPython_EXECUTABLE:PATH=/home/lerceg/rn-eestech/.venv/bin/python -DPython_ROOT_DIR:PATH=/home/lerceg/rn-eestech/.venv -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.11 -DPython3_EXECUTABLE:PATH=/home/lerceg/rn-eestech/.venv/bin/python -DPython3_ROOT_DIR:PATH=/home/lerceg/rn-eestech/.venv -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.11 -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-0w0ekvmf/overlay/lib/python3.11/site-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\r\n",
      "\r\n",
      "  Not searching for unused variables given on the command line.\r\n",
      "  -- The C compiler identification is GNU 13.2.0\r\n",
      "  -- The CXX compiler identification is GNU 13.2.0\r\n",
      "  -- Detecting C compiler ABI info\r\n",
      "  -- Detecting C compiler ABI info - done\r\n",
      "  -- Check for working C compiler: /usr/bin/cc - skipped\r\n",
      "  -- Detecting C compile features\r\n",
      "  -- Detecting C compile features - done\r\n",
      "  -- Detecting CXX compiler ABI info\r\n",
      "  -- Detecting CXX compiler ABI info - done\r\n",
      "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n",
      "  -- Detecting CXX compile features\r\n",
      "  -- Detecting CXX compile features - done\r\n",
      "  -- Found Git: /usr/bin/git (found version \"2.40.1\")\r\n",
      "  fatal: not a git repository (or any of the parent directories): .git\r\n",
      "  fatal: not a git repository (or any of the parent directories): .git\r\n",
      "  \u001B[33mCMake Warning at vendor/llama.cpp/CMakeLists.txt:117 (message):\r\n",
      "    Git repository not found; to enable automatic generation of build info,\r\n",
      "    make sure Git is installed and the project is a Git repository.\r\n",
      "\r\n",
      "  \u001B[0m\r\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\r\n",
      "  -- Found Threads: TRUE\r\n",
      "  -- Could not find nvcc, please set CUDAToolkit_ROOT.\r\n",
      "  \u001B[33mCMake Warning at vendor/llama.cpp/CMakeLists.txt:291 (message):\r\n",
      "    cuBLAS not found\r\n",
      "\r\n",
      "  \u001B[0m\r\n",
      "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\r\n",
      "  -- x86 detected\r\n",
      "  -- Configuring done (0.3s)\r\n",
      "  -- Generating done (0.0s)\r\n",
      "  -- Build files have been written to: /tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8/_skbuild/linux-x86_64-3.11/cmake-build\r\n",
      "  [1/8] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\r\n",
      "  [2/8] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o\r\n",
      "  [3/8] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\r\n",
      "  In file included from /usr/include/c++/13/string:51,\r\n",
      "                   from /tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8/vendor/llama.cpp/llama-util.h:15,\r\n",
      "                   from /tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8/vendor/llama.cpp/llama.cpp:9:\r\n",
      "  In static member function ‘static void std::__copy_move<false, false, std::random_access_iterator_tag>::__assign_one(_Tp*, _Up*) [with _Tp = const llama_grammar_element*; _Up = const llama_grammar_element* const]’,\r\n",
      "      inlined from ‘static _Up* std::__copy_move<_IsMove, true, std::random_access_iterator_tag>::__copy_m(_Tp*, _Tp*, _Up*) [with _Tp = const llama_grammar_element* const; _Up = const llama_grammar_element*; bool _IsMove = false]’ at /usr/include/c++/13/bits/stl_algobase.h:440:20,\r\n",
      "      inlined from ‘_OI std::__copy_move_a2(_II, _II, _OI) [with bool _IsMove = false; _II = const llama_grammar_element* const*; _OI = const llama_grammar_element**]’ at /usr/include/c++/13/bits/stl_algobase.h:506:30,\r\n",
      "      inlined from ‘_OI std::__copy_move_a1(_II, _II, _OI) [with bool _IsMove = false; _II = const llama_grammar_element* const*; _OI = const llama_grammar_element**]’ at /usr/include/c++/13/bits/stl_algobase.h:533:42,\r\n",
      "      inlined from ‘_OI std::__copy_move_a(_II, _II, _OI) [with bool _IsMove = false; _II = __gnu_cxx::__normal_iterator<const llama_grammar_element* const*, vector<const llama_grammar_element*> >; _OI = const llama_grammar_element**]’ at /usr/include/c++/13/bits/stl_algobase.h:540:31,\r\n",
      "      inlined from ‘_OI std::copy(_II, _II, _OI) [with _II = __gnu_cxx::__normal_iterator<const llama_grammar_element* const*, vector<const llama_grammar_element*> >; _OI = const llama_grammar_element**]’ at /usr/include/c++/13/bits/stl_algobase.h:633:7,\r\n",
      "      inlined from ‘static _ForwardIterator std::__uninitialized_copy<true>::__uninit_copy(_InputIterator, _InputIterator, _ForwardIterator) [with _InputIterator = __gnu_cxx::__normal_iterator<const llama_grammar_element* const*, std::vector<const llama_grammar_element*> >; _ForwardIterator = const llama_grammar_element**]’ at /usr/include/c++/13/bits/stl_uninitialized.h:147:27,\r\n",
      "      inlined from ‘_ForwardIterator std::uninitialized_copy(_InputIterator, _InputIterator, _ForwardIterator) [with _InputIterator = __gnu_cxx::__normal_iterator<const llama_grammar_element* const*, vector<const llama_grammar_element*> >; _ForwardIterator = const llama_grammar_element**]’ at /usr/include/c++/13/bits/stl_uninitialized.h:185:15,\r\n",
      "      inlined from ‘_ForwardIterator std::__uninitialized_copy_a(_InputIterator, _InputIterator, _ForwardIterator, allocator<_Tp>&) [with _InputIterator = __gnu_cxx::__normal_iterator<const llama_grammar_element* const*, vector<const llama_grammar_element*> >; _ForwardIterator = const llama_grammar_element**; _Tp = const llama_grammar_element*]’ at /usr/include/c++/13/bits/stl_uninitialized.h:373:37,\r\n",
      "      inlined from ‘std::vector<_Tp, _Alloc>::vector(const std::vector<_Tp, _Alloc>&) [with _Tp = const llama_grammar_element*; _Alloc = std::allocator<const llama_grammar_element*>]’ at /usr/include/c++/13/bits/stl_vector.h:603:31,\r\n",
      "      inlined from ‘void std::__new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = std::vector<const llama_grammar_element*>; _Args = {const std::vector<const llama_grammar_element*, std::allocator<const llama_grammar_element*> >&}; _Tp = std::vector<const llama_grammar_element*>]’ at /usr/include/c++/13/bits/new_allocator.h:187:4,\r\n",
      "      inlined from ‘static void std::allocator_traits<std::allocator<_CharT> >::construct(allocator_type&, _Up*, _Args&& ...) [with _Up = std::vector<const llama_grammar_element*>; _Args = {const std::vector<const llama_grammar_element*, std::allocator<const llama_grammar_element*> >&}; _Tp = std::vector<const llama_grammar_element*>]’ at /usr/include/c++/13/bits/alloc_traits.h:537:17,\r\n",
      "      inlined from ‘void std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = std::vector<const llama_grammar_element*>; _Alloc = std::allocator<std::vector<const llama_grammar_element*> >]’ at /usr/include/c++/13/bits/stl_vector.h:1283:30,\r\n",
      "      inlined from ‘void llama_grammar_advance_stack(const std::vector<std::vector<llama_grammar_element> >&, const std::vector<const llama_grammar_element*>&, std::vector<std::vector<const llama_grammar_element*> >&)’ at /tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8/vendor/llama.cpp/llama.cpp:2249:29:\r\n",
      "  /usr/include/c++/13/bits/stl_algobase.h:398:17: warning: array subscript 0 is outside array bounds of ‘const llama_grammar_element* [0]’ [-Warray-bounds=]\r\n",
      "    398 |         { *__to = *__from; }\r\n",
      "        |           ~~~~~~^~~~~~~~~\r\n",
      "  In file included from /usr/include/x86_64-linux-gnu/c++/13/bits/c++allocator.h:33,\r\n",
      "                   from /usr/include/c++/13/bits/allocator.h:46,\r\n",
      "                   from /usr/include/c++/13/string:43:\r\n",
      "  In member function ‘_Tp* std::__new_allocator<_Tp>::allocate(size_type, const void*) [with _Tp = const llama_grammar_element*]’,\r\n",
      "      inlined from ‘static _Tp* std::allocator_traits<std::allocator<_CharT> >::allocate(allocator_type&, size_type) [with _Tp = const llama_grammar_element*]’ at /usr/include/c++/13/bits/alloc_traits.h:482:28,\r\n",
      "      inlined from ‘std::_Vector_base<_Tp, _Alloc>::pointer std::_Vector_base<_Tp, _Alloc>::_M_allocate(std::size_t) [with _Tp = const llama_grammar_element*; _Alloc = std::allocator<const llama_grammar_element*>]’ at /usr/include/c++/13/bits/stl_vector.h:378:33,\r\n",
      "      inlined from ‘void std::_Vector_base<_Tp, _Alloc>::_M_create_storage(std::size_t) [with _Tp = const llama_grammar_element*; _Alloc = std::allocator<const llama_grammar_element*>]’ at /usr/include/c++/13/bits/stl_vector.h:395:44,\r\n",
      "      inlined from ‘std::_Vector_base<_Tp, _Alloc>::_Vector_base(std::size_t, const allocator_type&) [with _Tp = const llama_grammar_element*; _Alloc = std::allocator<const llama_grammar_element*>]’ at /usr/include/c++/13/bits/stl_vector.h:332:26,\r\n",
      "      inlined from ‘std::vector<_Tp, _Alloc>::vector(const std::vector<_Tp, _Alloc>&) [with _Tp = const llama_grammar_element*; _Alloc = std::allocator<const llama_grammar_element*>]’ at /usr/include/c++/13/bits/stl_vector.h:600:61,\r\n",
      "      inlined from ‘void std::__new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = std::vector<const llama_grammar_element*>; _Args = {const std::vector<const llama_grammar_element*, std::allocator<const llama_grammar_element*> >&}; _Tp = std::vector<const llama_grammar_element*>]’ at /usr/include/c++/13/bits/new_allocator.h:187:4,\r\n",
      "      inlined from ‘static void std::allocator_traits<std::allocator<_CharT> >::construct(allocator_type&, _Up*, _Args&& ...) [with _Up = std::vector<const llama_grammar_element*>; _Args = {const std::vector<const llama_grammar_element*, std::allocator<const llama_grammar_element*> >&}; _Tp = std::vector<const llama_grammar_element*>]’ at /usr/include/c++/13/bits/alloc_traits.h:537:17,\r\n",
      "      inlined from ‘void std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = std::vector<const llama_grammar_element*>; _Alloc = std::allocator<std::vector<const llama_grammar_element*> >]’ at /usr/include/c++/13/bits/stl_vector.h:1283:30,\r\n",
      "      inlined from ‘void llama_grammar_advance_stack(const std::vector<std::vector<llama_grammar_element> >&, const std::vector<const llama_grammar_element*>&, std::vector<std::vector<const llama_grammar_element*> >&)’ at /tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8/vendor/llama.cpp/llama.cpp:2249:29:\r\n",
      "  /usr/include/c++/13/bits/new_allocator.h:147:55: note: object of size 0 allocated by ‘operator new’\r\n",
      "    147 |         return static_cast<_Tp*>(_GLIBCXX_OPERATOR_NEW(__n * sizeof(_Tp)));\r\n",
      "        |                                                       ^\r\n",
      "  [4/8] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\r\n",
      "  [5/8] Linking C static library vendor/llama.cpp/libggml_static.a\r\n",
      "  [6/8] Linking C shared library vendor/llama.cpp/libggml_shared.so\r\n",
      "  [7/8] Linking CXX shared library vendor/llama.cpp/libllama.so\r\n",
      "  [7/8] Install the project...\r\n",
      "  -- Install configuration: \"Release\"\r\n",
      "  -- Installing: /tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8/_skbuild/linux-x86_64-3.11/cmake-install/lib/libggml_shared.so\r\n",
      "  -- Installing: /tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8/_skbuild/linux-x86_64-3.11/cmake-install/lib/libllama.so\r\n",
      "  -- Installing: /tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8/_skbuild/linux-x86_64-3.11/cmake-install/bin/convert.py\r\n",
      "  -- Installing: /tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8/_skbuild/linux-x86_64-3.11/cmake-install/bin/convert-lora-to-ggml.py\r\n",
      "  -- Installing: /tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8/_skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/libllama.so\r\n",
      "\r\n",
      "  copying llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/llama_grammar.py\r\n",
      "  copying llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/llama_types.py\r\n",
      "  copying llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/__init__.py\r\n",
      "  copying llama_cpp/utils.py -> _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/utils.py\r\n",
      "  copying llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/llama_cpp.py\r\n",
      "  copying llama_cpp/llama.py -> _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/llama.py\r\n",
      "  creating directory _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/server\r\n",
      "  copying llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/server/app.py\r\n",
      "  copying llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/server/__main__.py\r\n",
      "  copying llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/server/__init__.py\r\n",
      "  copying /tmp/pip-install-sitpzbh3/llama-cpp-python_535a1b4abb8e4f539eef670b031d4cf8/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/py.typed\r\n",
      "\r\n",
      "  running bdist_wheel\r\n",
      "  running build\r\n",
      "  running build_py\r\n",
      "  creating _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311\r\n",
      "  creating _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  creating _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/server\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/server\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/server\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/server\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/server\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/server\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/server\r\n",
      "  copied 9 files\r\n",
      "  running build_ext\r\n",
      "  installing to _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel\r\n",
      "  running install\r\n",
      "  running install_lib\r\n",
      "  creating _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64\r\n",
      "  creating _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel\r\n",
      "  creating _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp\r\n",
      "  creating _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\r\n",
      "  copying _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\r\n",
      "  copying _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\r\n",
      "  copying _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\r\n",
      "  copying _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp\r\n",
      "  copying _skbuild/linux-x86_64-3.11/setuptools/lib.linux-x86_64-cpython-311/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp\r\n",
      "  copied 11 files\r\n",
      "  running install_data\r\n",
      "  creating _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data\r\n",
      "  creating _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data\r\n",
      "  creating _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/lib/libggml_shared.so -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/lib/libllama.so -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\r\n",
      "  creating _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/bin/convert.py -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\r\n",
      "  copying _skbuild/linux-x86_64-3.11/cmake-install/bin/convert-lora-to-ggml.py -> _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\r\n",
      "  running install_egg_info\r\n",
      "  running egg_info\r\n",
      "  writing llama_cpp_python.egg-info/PKG-INFO\r\n",
      "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\r\n",
      "  writing requirements to llama_cpp_python.egg-info/requires.txt\r\n",
      "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\r\n",
      "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\r\n",
      "  adding license file 'LICENSE.md'\r\n",
      "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\r\n",
      "  Copying llama_cpp_python.egg-info to _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78-py3.11.egg-info\r\n",
      "  running install_scripts\r\n",
      "  copied 0 files\r\n",
      "  creating _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.dist-info/WHEEL\r\n",
      "  creating '/tmp/pip-wheel-af1f_a8a/.tmp-eq_1ybta/llama_cpp_python-0.1.78-cp311-cp311-linux_x86_64.whl' and adding '_skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel' to it\r\n",
      "  adding 'llama_cpp/__init__.py'\r\n",
      "  adding 'llama_cpp/libllama.so'\r\n",
      "  adding 'llama_cpp/llama.py'\r\n",
      "  adding 'llama_cpp/llama_cpp.py'\r\n",
      "  adding 'llama_cpp/llama_grammar.py'\r\n",
      "  adding 'llama_cpp/llama_types.py'\r\n",
      "  adding 'llama_cpp/py.typed'\r\n",
      "  adding 'llama_cpp/utils.py'\r\n",
      "  adding 'llama_cpp/server/__init__.py'\r\n",
      "  adding 'llama_cpp/server/__main__.py'\r\n",
      "  adding 'llama_cpp/server/app.py'\r\n",
      "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert-lora-to-ggml.py'\r\n",
      "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert.py'\r\n",
      "  adding 'llama_cpp_python-0.1.78.data/data/lib/libggml_shared.so'\r\n",
      "  adding 'llama_cpp_python-0.1.78.data/data/lib/libllama.so'\r\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/LICENSE.md'\r\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/METADATA'\r\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/WHEEL'\r\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/top_level.txt'\r\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/RECORD'\r\n",
      "  removing _skbuild/linux-x86_64-3.11/setuptools/bdist.linux-x86_64/wheel\r\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\r\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp311-cp311-linux_x86_64.whl size=755446 sha256=2a982108d30b4002734190a29e3371d9bab83500e3c7e4fbb070dfef7ddcaa4c\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-kzpfera4/wheels/18/69/ad/9b1cec6b18fe403616b8cfbf10021d1939cba077cee285c5c3\r\n",
      "Successfully built llama-cpp-python\r\n",
      "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.11.0\r\n",
      "    Uninstalling typing_extensions-4.11.0:\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/lib/python3.11/site-packages/__pycache__/typing_extensions.cpython-311.pyc\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/lib/python3.11/site-packages/typing_extensions-4.11.0.dist-info/\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/lib/python3.11/site-packages/typing_extensions.py\r\n",
      "      Successfully uninstalled typing_extensions-4.11.0\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.23.4\r\n",
      "    Uninstalling numpy-1.23.4:\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/bin/f2py\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/bin/f2py3\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/bin/f2py3.11\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/lib/python3.11/site-packages/numpy-1.23.4.dist-info/\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/lib/python3.11/site-packages/numpy.libs/\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/lib/python3.11/site-packages/numpy/\r\n",
      "      Successfully uninstalled numpy-1.23.4\r\n",
      "  changing mode of /home/lerceg/rn-eestech/.venv/bin/f2py to 775\r\n",
      "  changing mode of /home/lerceg/rn-eestech/.venv/bin/f2py3 to 775\r\n",
      "  changing mode of /home/lerceg/rn-eestech/.venv/bin/f2py3.11 to 775\r\n",
      "  Attempting uninstall: diskcache\r\n",
      "    Found existing installation: diskcache 5.6.3\r\n",
      "    Uninstalling diskcache-5.6.3:\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/lib/python3.11/site-packages/diskcache-5.6.3.dist-info/\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/lib/python3.11/site-packages/diskcache/\r\n",
      "      Successfully uninstalled diskcache-5.6.3\r\n",
      "  Attempting uninstall: llama-cpp-python\r\n",
      "    Found existing installation: llama_cpp_python 0.1.78\r\n",
      "    Uninstalling llama_cpp_python-0.1.78:\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/bin/__pycache__/\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/bin/convert-lora-to-ggml.py\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/bin/convert.py\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/lib/libggml_shared.so\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/lib/libllama.so\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/lib/python3.11/site-packages/llama_cpp/\r\n",
      "      Removing file or directory /home/lerceg/rn-eestech/.venv/lib/python3.11/site-packages/llama_cpp_python-0.1.78.dist-info/\r\n",
      "      Successfully uninstalled llama_cpp_python-0.1.78\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "trl 0.8.6 requires transformers>=4.31.0, but you have transformers 4.30.0 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.23.4 typing-extensions-4.11.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.11/site-packages (0.22.2)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (3.13.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (2024.3.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (24.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface_hub) (6.0.1)\r\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from huggingface_hub) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (4.66.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (4.11.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: llama-cpp-python==0.1.78 in ./.venv/lib/python3.11/site-packages (0.1.78)\r\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from llama-cpp-python==0.1.78) (4.11.0)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in ./.venv/lib/python3.11/site-packages (from llama-cpp-python==0.1.78) (1.23.4)\r\n",
      "Requirement already satisfied: diskcache>=5.6.1 in ./.venv/lib/python3.11/site-packages (from llama-cpp-python==0.1.78) (5.6.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: numpy==1.23.4 in ./.venv/lib/python3.11/site-packages (1.23.4)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
   ],
   "metadata": {
    "id": "qJ90LnMv54Y-",
    "ExecuteTime": {
     "end_time": "2024-04-27T16:57:53.161130Z",
     "start_time": "2024-04-27T16:57:53.158436Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "#**Step 2: Import All the Required Libraries**"
   ],
   "metadata": {
    "id": "6lOmpKB36RJh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama"
   ],
   "metadata": {
    "id": "Ak3ZtGjM6Wdp",
    "ExecuteTime": {
     "end_time": "2024-04-27T16:57:54.077526Z",
     "start_time": "2024-04-27T16:57:53.858466Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "#**Step 3: Download the Model**"
   ],
   "metadata": {
    "id": "haAb9kNm6J9n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ba1bcb9ff17f4553a81db1e785809888",
      "c60f6cadc2c0473291bb31561da8aa75",
      "1e34c1ae66b0467c9661c11d4da53fee",
      "784514e8b24c41749273b51c9910ac88",
      "76b3ffa319034c47a1aeab12ac3fad2e",
      "ae1d58b642c94419a1a28856cb11ddc3",
      "dbbd63c1b01549198ec9b797bb55c80e",
      "c76ef761f92449138197258ce57ef88b",
      "7e375052eea04371b6698a608139a915",
      "1b92920245f0425c8ea88e17d47d5232",
      "f043188c15454e90bd3c93cfd55d33bd"
     ]
    },
    "id": "qBgdGV4b6MxG",
    "outputId": "7763d6e6-bd2b-4158-b0a6-f0143b436ff1",
    "ExecuteTime": {
     "end_time": "2024-04-27T17:55:05.687052Z",
     "start_time": "2024-04-27T16:57:54.588775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama-2-13b-chat.ggmlv3.q5_1.bin:   0%|          | 0.00/9.76G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "911562c58d014eaabc217d54e5d45cac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.huggingface.co/repos/cd/43/cd4356b11767f5136b31b27dbb8863d6dd69a4010e034ef75be9c2c12fcd10f7/97d9becd5a364323c7959cc82e7506d6eb26c025623320b844e45e517e3dfe76?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.ggmlv3.q5_1.bin%3B+filename%3D%22llama-2-13b-chat.ggmlv3.q5_1.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1714496275&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDQ5NjI3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jZC80My9jZDQzNTZiMTE3NjdmNTEzNmIzMWIyN2RiYjg4NjNkNmRkNjlhNDAxMGUwMzRlZjc1YmU5YzJjMTJmY2QxMGY3Lzk3ZDliZWNkNWEzNjQzMjNjNzk1OWNjODJlNzUwNmQ2ZWIyNmMwMjU2MjMzMjBiODQ0ZTQ1ZTUxN2UzZGZlNzY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=S%7Es%7EJZ9DQsYGjqEKZAqcPiZ9-g7BVvSk%7EDKnXgcLBccvBSaI4BnQJG5PAmkYyXpQ-NMapUkzjgd-8ZTP7cY1zZSv9%7EtNlO%7EHQP1nN2enTvUGBNYwyBNduc2FfGWX5ekUX5bSZ3fYJ-HS17V-055nQavs%7EB-grwsbJSDvmgmKL3OyJYh7Fq0KOsOaIe2NNiVGi7Epok35z5yE7MN6pS0S-adgqp8mg-vjFI6TZAxOwbpA2CITw5UjfjDX5hGX3DjuKyBfkL1MCc6In%7EFMA17mF5X-U0CWed9uAdp8lYY5mL8GM1rpx7ijWYYntouqxbYt5JckUWk9oIm%7EMog88G%7ES0Q__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "llama-2-13b-chat.ggmlv3.q5_1.bin:   2%|2         | 210M/9.76G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11ba14702d794c45b55aa741af38fbf0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.huggingface.co/repos/cd/43/cd4356b11767f5136b31b27dbb8863d6dd69a4010e034ef75be9c2c12fcd10f7/97d9becd5a364323c7959cc82e7506d6eb26c025623320b844e45e517e3dfe76?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.ggmlv3.q5_1.bin%3B+filename%3D%22llama-2-13b-chat.ggmlv3.q5_1.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1714496275&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDQ5NjI3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jZC80My9jZDQzNTZiMTE3NjdmNTEzNmIzMWIyN2RiYjg4NjNkNmRkNjlhNDAxMGUwMzRlZjc1YmU5YzJjMTJmY2QxMGY3Lzk3ZDliZWNkNWEzNjQzMjNjNzk1OWNjODJlNzUwNmQ2ZWIyNmMwMjU2MjMzMjBiODQ0ZTQ1ZTUxN2UzZGZlNzY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=S%7Es%7EJZ9DQsYGjqEKZAqcPiZ9-g7BVvSk%7EDKnXgcLBccvBSaI4BnQJG5PAmkYyXpQ-NMapUkzjgd-8ZTP7cY1zZSv9%7EtNlO%7EHQP1nN2enTvUGBNYwyBNduc2FfGWX5ekUX5bSZ3fYJ-HS17V-055nQavs%7EB-grwsbJSDvmgmKL3OyJYh7Fq0KOsOaIe2NNiVGi7Epok35z5yE7MN6pS0S-adgqp8mg-vjFI6TZAxOwbpA2CITw5UjfjDX5hGX3DjuKyBfkL1MCc6In%7EFMA17mF5X-U0CWed9uAdp8lYY5mL8GM1rpx7ijWYYntouqxbYt5JckUWk9oIm%7EMog88G%7ES0Q__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "llama-2-13b-chat.ggmlv3.q5_1.bin:   3%|3         | 336M/9.76G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d307205fa704f73b6b38dc0bba66039"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.huggingface.co/repos/cd/43/cd4356b11767f5136b31b27dbb8863d6dd69a4010e034ef75be9c2c12fcd10f7/97d9becd5a364323c7959cc82e7506d6eb26c025623320b844e45e517e3dfe76?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.ggmlv3.q5_1.bin%3B+filename%3D%22llama-2-13b-chat.ggmlv3.q5_1.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1714496275&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDQ5NjI3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jZC80My9jZDQzNTZiMTE3NjdmNTEzNmIzMWIyN2RiYjg4NjNkNmRkNjlhNDAxMGUwMzRlZjc1YmU5YzJjMTJmY2QxMGY3Lzk3ZDliZWNkNWEzNjQzMjNjNzk1OWNjODJlNzUwNmQ2ZWIyNmMwMjU2MjMzMjBiODQ0ZTQ1ZTUxN2UzZGZlNzY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=S%7Es%7EJZ9DQsYGjqEKZAqcPiZ9-g7BVvSk%7EDKnXgcLBccvBSaI4BnQJG5PAmkYyXpQ-NMapUkzjgd-8ZTP7cY1zZSv9%7EtNlO%7EHQP1nN2enTvUGBNYwyBNduc2FfGWX5ekUX5bSZ3fYJ-HS17V-055nQavs%7EB-grwsbJSDvmgmKL3OyJYh7Fq0KOsOaIe2NNiVGi7Epok35z5yE7MN6pS0S-adgqp8mg-vjFI6TZAxOwbpA2CITw5UjfjDX5hGX3DjuKyBfkL1MCc6In%7EFMA17mF5X-U0CWed9uAdp8lYY5mL8GM1rpx7ijWYYntouqxbYt5JckUWk9oIm%7EMog88G%7ES0Q__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "llama-2-13b-chat.ggmlv3.q5_1.bin:   4%|3         | 357M/9.76G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67a9c054b6bf43b29c0a2907a0a236f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.huggingface.co/repos/cd/43/cd4356b11767f5136b31b27dbb8863d6dd69a4010e034ef75be9c2c12fcd10f7/97d9becd5a364323c7959cc82e7506d6eb26c025623320b844e45e517e3dfe76?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.ggmlv3.q5_1.bin%3B+filename%3D%22llama-2-13b-chat.ggmlv3.q5_1.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1714496275&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDQ5NjI3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jZC80My9jZDQzNTZiMTE3NjdmNTEzNmIzMWIyN2RiYjg4NjNkNmRkNjlhNDAxMGUwMzRlZjc1YmU5YzJjMTJmY2QxMGY3Lzk3ZDliZWNkNWEzNjQzMjNjNzk1OWNjODJlNzUwNmQ2ZWIyNmMwMjU2MjMzMjBiODQ0ZTQ1ZTUxN2UzZGZlNzY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=S%7Es%7EJZ9DQsYGjqEKZAqcPiZ9-g7BVvSk%7EDKnXgcLBccvBSaI4BnQJG5PAmkYyXpQ-NMapUkzjgd-8ZTP7cY1zZSv9%7EtNlO%7EHQP1nN2enTvUGBNYwyBNduc2FfGWX5ekUX5bSZ3fYJ-HS17V-055nQavs%7EB-grwsbJSDvmgmKL3OyJYh7Fq0KOsOaIe2NNiVGi7Epok35z5yE7MN6pS0S-adgqp8mg-vjFI6TZAxOwbpA2CITw5UjfjDX5hGX3DjuKyBfkL1MCc6In%7EFMA17mF5X-U0CWed9uAdp8lYY5mL8GM1rpx7ijWYYntouqxbYt5JckUWk9oIm%7EMog88G%7ES0Q__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "llama-2-13b-chat.ggmlv3.q5_1.bin:  53%|#####2    | 5.14G/9.76G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d17ed3b606f64807966db3a869b0bc34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.huggingface.co/repos/cd/43/cd4356b11767f5136b31b27dbb8863d6dd69a4010e034ef75be9c2c12fcd10f7/97d9becd5a364323c7959cc82e7506d6eb26c025623320b844e45e517e3dfe76?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.ggmlv3.q5_1.bin%3B+filename%3D%22llama-2-13b-chat.ggmlv3.q5_1.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1714496275&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDQ5NjI3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jZC80My9jZDQzNTZiMTE3NjdmNTEzNmIzMWIyN2RiYjg4NjNkNmRkNjlhNDAxMGUwMzRlZjc1YmU5YzJjMTJmY2QxMGY3Lzk3ZDliZWNkNWEzNjQzMjNjNzk1OWNjODJlNzUwNmQ2ZWIyNmMwMjU2MjMzMjBiODQ0ZTQ1ZTUxN2UzZGZlNzY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=S%7Es%7EJZ9DQsYGjqEKZAqcPiZ9-g7BVvSk%7EDKnXgcLBccvBSaI4BnQJG5PAmkYyXpQ-NMapUkzjgd-8ZTP7cY1zZSv9%7EtNlO%7EHQP1nN2enTvUGBNYwyBNduc2FfGWX5ekUX5bSZ3fYJ-HS17V-055nQavs%7EB-grwsbJSDvmgmKL3OyJYh7Fq0KOsOaIe2NNiVGi7Epok35z5yE7MN6pS0S-adgqp8mg-vjFI6TZAxOwbpA2CITw5UjfjDX5hGX3DjuKyBfkL1MCc6In%7EFMA17mF5X-U0CWed9uAdp8lYY5mL8GM1rpx7ijWYYntouqxbYt5JckUWk9oIm%7EMog88G%7ES0Q__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "llama-2-13b-chat.ggmlv3.q5_1.bin:  67%|######6   | 6.51G/9.76G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e26a476ace841a092e60f1063ad523b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "#**Step 4: Loading the Model**"
   ],
   "metadata": {
    "id": "VQ6OYnI46kKq"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T20:09:11.300343Z",
     "start_time": "2024-04-27T20:09:11.297415Z"
    }
   },
   "cell_type": "code",
   "source": "model_path",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lerceg/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "source": [
    "# GPU\n",
    "lcpp_llm = None\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2, # CPU cores\n",
    "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",
    "    )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irftToUj6aWt",
    "outputId": "35ed7af0-5268-46f9-cdad-3406515d114c",
    "ExecuteTime": {
     "end_time": "2024-04-27T18:01:03.578919Z",
     "start_time": "2024-04-27T18:01:02.949683Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/lerceg/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 9311.07 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   75.35 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# See the number of layers in GPU\n",
    "lcpp_llm.params.n_gpu_layers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YG4Pylz662At",
    "outputId": "1c0774fc-ca6a-447e-e564-bb2ce4fd25d6",
    "ExecuteTime": {
     "end_time": "2024-04-27T18:01:08.561905Z",
     "start_time": "2024-04-27T18:01:08.558935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "#**Step 5: Create a Prompt Template**"
   ],
   "metadata": {
    "id": "iE-M307R6_pT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = (\n",
    "\"\"\"Here are the sentences \\n\n",
    "The study material is very accessible. \\n\n",
    "\"Cleanliness is not acceptable. \\n\n",
    "\"Keyboards in classrooms are not working properly.\\n\n",
    "\"Sometimes professor shows signs of anger issues.\\n\n",
    "\"Library doesnt have all proper literature.\\n\n",
    "\"Teaching assistants can be very helpful.\\n\n",
    "\"Professor Stojakovic is the best.\\n\n",
    "          \"\"\")\n",
    "prompt_template=f'''SYSTEM: You will need to classify student feedback for each sentence give the output in format (sentiment, topic). Sentiment can either be POSITIVE or NEGATIVE. Topic can be choosen as one from this list is either positive or negative and 'topic' which should be te subject of the feedback. It can be one of the following: ['professor, 'assistant', 'study material', 'infrastructure', 'course work'], if you cannot confidently classify the topic into one of those classify as 'other'. Please only return \n",
    "\n",
    "ASSISTANT:\n",
    "\n",
    "USER: {prompt}\n",
    "\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "id": "RfzwELMC7Dyg",
    "ExecuteTime": {
     "end_time": "2024-04-27T19:44:00.997366Z",
     "start_time": "2024-04-27T19:44:00.995126Z"
    }
   },
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "#**Step 6: Generating the Response**"
   ],
   "metadata": {
    "id": "aT8pg6zt7QzA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "response=lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,\n",
    "                  repeat_penalty=1.2, top_k=20,\n",
    "                  echo=True)"
   ],
   "metadata": {
    "id": "0aF0qWUJ7OPK",
    "ExecuteTime": {
     "end_time": "2024-04-27T19:46:49.075758Z",
     "start_time": "2024-04-27T19:44:04.543711Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12804.72 ms\n",
      "llama_print_timings:      sample time =   127.49 ms /   256 runs   (    0.50 ms per token,  2007.97 tokens per second)\n",
      "llama_print_timings: prompt eval time = 54055.39 ms /   159 tokens (  339.97 ms per token,     2.94 tokens per second)\n",
      "llama_print_timings:        eval time = 109903.84 ms /   255 runs   (  431.00 ms per token,     2.32 tokens per second)\n",
      "llama_print_timings:       total time = 164527.37 ms\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "source": [
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jlJ1JgR68DDO",
    "outputId": "297c4d78-d48b-40a1-b375-116974153b5e",
    "ExecuteTime": {
     "end_time": "2024-04-27T19:46:52.996182Z",
     "start_time": "2024-04-27T19:46:52.993662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-3b58dfcd-dd46-4c1c-bd05-50ed6d08d670', 'object': 'text_completion', 'created': 1714247044, 'model': '/home/lerceg/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin', 'choices': [{'text': 'SYSTEM: You will need to classify student feedback for each sentence give a json that contains two keys \\'sentiment\\' which is either positive or negative and \\'topic\\' which should be te subject of the feedback. It can be one of the following: [\\'proffesors, \\'assistants\\', \\'study material\\', \\'infrastructure\\', \\'course work\\'], if you cannot confidently classify the topic into one of those classify as \\'other\\'.\\n\\nASSISTANT:\\n\\nUSER: Here are the sentences \\n\\nThe study material is very acessable. \\n\\n\"Cleanliness is not acceptable. \\n\\n\"Keyboards in classrooms are not working properly.\\n\\n\"Sometimes professor shows signs of anger issues.\\n\\n\"Library doesnt have all proper literature.\\n\\n\"Teaching assistants can be very helpful.\\n\\n\"Professor Stojakovic is the best.\\n\\n          \\n\\n\\nPlease provide feedback for each sentence and classify them into sentiment and topic.\\n\\nASSISTANT: Sure! Here are my assessments of your sentences, along with their sentiment and topics:\\n\\n1. \"The study material is very accessible.\"\\nSentiment: Positive Topic: Study Material\\n2. \"Cleanliness is not acceptable.\"\\nSentiment: Negative Topic: Infrastructure\\n3. \"Keyboards in classrooms are not working properly.\"\\nSentiment: Negative Topic: Infrastructure\\n4. \"Sometimes professor shows signs of anger issues.\"\\nSentiment: Negative Topic: Professors\\n5. \"Library doesn\\'t have all proper literature.\"\\nSentiment: Negative Topic: Study Material\\n6. \"Teaching assistants can be very helpful.\"\\nSentiment: Positive Topic: Teaching Assistants\\n7. \"Professor Stojakovic is the best.\"\\nSentiment: Positive Topic: Professors\\n\\nPlease note that for sentence 5, I have classified it as \\'Other\\' because it does not fit neatly into any of the predefined topics', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 205, 'completion_tokens': 256, 'total_tokens': 461}}\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "source": "print(response[\"choices\"][0]['text'])",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qona58gX8oAn",
    "outputId": "3eead385-f77a-469c-bcb5-3d9adccbc8db",
    "ExecuteTime": {
     "end_time": "2024-04-27T19:46:53.742169Z",
     "start_time": "2024-04-27T19:46:53.739610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You will need to classify student feedback for each sentence give a json that contains two keys 'sentiment' which is either positive or negative and 'topic' which should be te subject of the feedback. It can be one of the following: ['proffesors, 'assistants', 'study material', 'infrastructure', 'course work'], if you cannot confidently classify the topic into one of those classify as 'other'.\n",
      "\n",
      "ASSISTANT:\n",
      "\n",
      "USER: Here are the sentences \n",
      "\n",
      "The study material is very acessable. \n",
      "\n",
      "\"Cleanliness is not acceptable. \n",
      "\n",
      "\"Keyboards in classrooms are not working properly.\n",
      "\n",
      "\"Sometimes professor shows signs of anger issues.\n",
      "\n",
      "\"Library doesnt have all proper literature.\n",
      "\n",
      "\"Teaching assistants can be very helpful.\n",
      "\n",
      "\"Professor Stojakovic is the best.\n",
      "\n",
      "          \n",
      "\n",
      "\n",
      "Please provide feedback for each sentence and classify them into sentiment and topic.\n",
      "\n",
      "ASSISTANT: Sure! Here are my assessments of your sentences, along with their sentiment and topics:\n",
      "\n",
      "1. \"The study material is very accessible.\"\n",
      "Sentiment: Positive Topic: Study Material\n",
      "2. \"Cleanliness is not acceptable.\"\n",
      "Sentiment: Negative Topic: Infrastructure\n",
      "3. \"Keyboards in classrooms are not working properly.\"\n",
      "Sentiment: Negative Topic: Infrastructure\n",
      "4. \"Sometimes professor shows signs of anger issues.\"\n",
      "Sentiment: Negative Topic: Professors\n",
      "5. \"Library doesn't have all proper literature.\"\n",
      "Sentiment: Negative Topic: Study Material\n",
      "6. \"Teaching assistants can be very helpful.\"\n",
      "Sentiment: Positive Topic: Teaching Assistants\n",
      "7. \"Professor Stojakovic is the best.\"\n",
      "Sentiment: Positive Topic: Professors\n",
      "\n",
      "Please note that for sentence 5, I have classified it as 'Other' because it does not fit neatly into any of the predefined topics\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "source": "\"vanja je vanja\".find('anja')",
   "metadata": {
    "id": "N4uMV0zF8pQt",
    "ExecuteTime": {
     "end_time": "2024-04-27T20:44:57.061961Z",
     "start_time": "2024-04-27T20:44:57.058744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:41:29.308510Z",
     "start_time": "2024-04-27T22:41:29.306022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response_text = \"\"\"\"ASSISTANT:\n",
    "        1. The toilets are clean - POSITIVE, other\n",
    "        2. The staff is good - POSITIVE, other\n",
    "        3. Professors are not so good - NEGATIVE, professor\n",
    "        4. Classes could be more interactive - NEUTRAL, study material\n",
    "        SYSTEM: Here is the feedback : The toilets are clean. The staff is good. Professors are not so good. Classes could be more interactive. My output would be (sentiment, topic) = ((POSITIVE, other), (POSITIVE, othe\" \n",
    "        \"\"\""
   ],
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:31:43.725134Z",
     "start_time": "2024-04-27T22:31:43.722920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assistant_first = response_text.find(\"ASSISTANT:\")\n",
    "user_next = response_text.find(\"USER:\", assistant_first)\n",
    "system_next = response_text.find(\"SYSTEM:\", assistant_first)\n"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:38:29.296Z",
     "start_time": "2024-04-27T22:38:29.292919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reponses = []\n",
    "for x in response_text.split(\"\\n\")[1 : -1]:\n",
    "    sentence = \" \".join(x.strip().split(' - (')[0].split(' ')[1 : ]).strip() + \".\"\n",
    "    sentiment, topic = x.strip().split(' - (')[1].split(',')\n",
    "    sentiment = sentiment.strip().replace('(', '').replace(')', '')\n",
    "    topic = topic.strip().replace('(', '').replace(')', '')\n",
    "    dict = {'sentence' : sentence, 'sentiment': sentiment, 'topic': topic}\n",
    "    reponses.append(dict)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:38:29.485090Z",
     "start_time": "2024-04-27T22:38:29.481857Z"
    }
   },
   "cell_type": "code",
   "source": "reponses",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'Classrooms are old.', 'sentiment': '-', 'topic': 'other'},\n",
       " {'sentence': 'Desks are new and pretty.',\n",
       "  'sentiment': 'positive',\n",
       "  'topic': 'study material'},\n",
       " {'sentence': 'The infrastructure of the building is getting old.',\n",
       "  'sentiment': 'negative',\n",
       "  'topic': 'infrastructure'},\n",
       " {'sentence': 'Professors are excellent at teaching!.',\n",
       "  'sentiment': 'positive',\n",
       "  'topic': 'professor'}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:54:01.877922Z",
     "start_time": "2024-04-27T22:54:01.873888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_output(raw : str) -> list:\n",
    "    assistant_first = raw.find(\"ASSISTANT:\")\n",
    "    user_next = raw.find(\"USER:\", assistant_first)\n",
    "    user_next = user_next if user_next >= 0 else len(raw)\n",
    "    system_next = raw.find(\"SYSTEM:\", assistant_first)\n",
    "    system_next = system_next if system_next >= 0 else len(raw)\n",
    "\n",
    "    end = min(user_next, system_next)\n",
    "\n",
    "    raw_cut = raw[assistant_first: end]\n",
    "\n",
    "    responses = []\n",
    "    for x in raw_cut.split(\"\\n\")[1: -1]:\n",
    "        sentence = \" \".join(x.strip().split(' - (')[0].split(' ')[1:]).strip() + \".\"\n",
    "        sentiment, topic = x.strip().split(' - (')[1].split(',')\n",
    "        sentiment = sentiment.strip().replace('(', '').replace(')', '')\n",
    "        topic = topic.strip().replace('(', '').replace(')', '')\n",
    "        dict = {'sentence': sentence, 'sentiment': sentiment, 'topic': topic}\n",
    "        responses.append(dict)\n",
    "    return reponses\n"
   ],
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:41:32.691372Z",
     "start_time": "2024-04-27T22:41:32.662388Z"
    }
   },
   "cell_type": "code",
   "source": "prepare_output(response_text)",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[104], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mprepare_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse_text\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[102], line 15\u001B[0m, in \u001B[0;36mprepare_output\u001B[0;34m(raw)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m raw_cut\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m1\u001B[39m: \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m     14\u001B[0m     sentence \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(x\u001B[38;5;241m.\u001B[39mstrip()\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m - (\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m1\u001B[39m:])\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 15\u001B[0m     sentiment, topic \u001B[38;5;241m=\u001B[39m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrip\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m - (\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     16\u001B[0m     sentiment \u001B[38;5;241m=\u001B[39m sentiment\u001B[38;5;241m.\u001B[39mstrip()\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     17\u001B[0m     topic \u001B[38;5;241m=\u001B[39m topic\u001B[38;5;241m.\u001B[39mstrip()\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:54:04.394643Z",
     "start_time": "2024-04-27T22:54:04.392237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response_text = \"\"\"SYSTEM: You will need to classify student feedback for each sentence give the output in format (sentiment, topic). Sentiment can either be POSITIVE or NEGATIVE. Topic can be choosen as one from this list is either positive or negative and 'topic' which should be te subject of the feedback. It can be one of the following: ['professor, 'assistant', 'study material', 'infrastructure', 'course work'], if you cannot confidently classify the topic into one of those classify as 'other'. Please only return topics that i specified. Please for each sentecene, show output in format \"n. sentence - (sentiment, topic)\".  \n",
    "        USER: Here is the feedback : Toilets are not nice. Professors guide really well. Chairs are little older then they should be. Infrastructure is holding quite well.\n",
    "        ASSISTANT:\n",
    "        1. Toilets are not nice - (NEGATIVE, other)\n",
    "        2. Professors guide really well - (POSITIVE, professor)\n",
    "        3. Chairs are little older then they should be - (NEGATIVE, infrastructure)\n",
    "        4. Infrastructure is holding quite well - (POSITIVE, infrastructure) \"\"\""
   ],
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:54:04.900589Z",
     "start_time": "2024-04-27T22:54:04.897556Z"
    }
   },
   "cell_type": "code",
   "source": "prepare_output(response_text)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'Classrooms are old.', 'sentiment': '-', 'topic': 'other'},\n",
       " {'sentence': 'Desks are new and pretty.',\n",
       "  'sentiment': 'positive',\n",
       "  'topic': 'study material'},\n",
       " {'sentence': 'The infrastructure of the building is getting old.',\n",
       "  'sentiment': 'negative',\n",
       "  'topic': 'infrastructure'},\n",
       " {'sentence': 'Professors are excellent at teaching!.',\n",
       "  'sentiment': 'positive',\n",
       "  'topic': 'professor'}]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
